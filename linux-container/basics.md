cgroups manage resources for groups of processes.
namespaces per process resource isolation.

seccomp limit available system calls
capabilities limit available privileges
CRIU checkpoint/restore (with kernel support)

# cgroups - user space view

Low-level filesystem interface similar to sysfs (/sys) and procfs (/proc) new filesystem type "cgroup", default location in /sys/fs/cgroup.

**cgroup hierarchies**
/sys/fs/cgroup
/cpu
/mem : memory

**subsystems controllers**
each subsystem can be used at most once or, if a new top-level cgroup is created with an already existing combination of subsystems, the previous top-level cgroup will be used behind the scenes.

**common**
tasks

cgroup.procs

release_agent

notify_on_release

cgroup.clone_children

cgroup.sane_behavior

​**cpuacct**
cpuacct.stat

cpuacct.usage

cpuacct.usage_percpu

**cpu**
cpu.stat

cpu.shares

cpu.cfs_period_us

cpu.cfs_quota_us

cpu.rt_period_us

cpu.rt_runtime_us

​by default, the top-level cgroup contains all running tasks

- release_agent: is only present at the top-level cgroup level, and contains a command to be run when the last process of a cgroup terminates.
- notify_on_release: needs to be set in particular cgroups for that command to actually execute.
- cpu controller: by default, the kernel scheduler aims to give equal cpu time to all processes. cgroups can be used for fair grouping between arbitrary sets of processes (an example of 30 apache processes and 10 postgres processes)
- net_cls: interface for tagging network packets with a class identifier (so that you could later add rules based on packet class)
- memory controller: has hierarchical support and allows for soft limits (cgroup can use as much memory as needed provided there is no memory contention and the hard limit is not exceeded).
Hierarchical support means that child cgroups contribute to the memory usage of their ancestors. If an ancestor exceeds a limit, memory will be reclaimed from the ancestor and all its children

# namespaces -user view
Namespaces limit the scope of kernel-side "names" and "data structures" at at process granularity.
mnt (mount points, filesystems)
pid(processes)
net(network stack)
ipc(System V IPC)
uts(unix timesharing - domaine name, etc)
user(UIDs)

Three system calls for management
clone() new process, new namespace, attach process to namespace.
unshare() new namespace, attach current process to it.
setns(int fd, int nstype) join an existing namespace.

Each namespace is identified by an inode (unique)

# Containers

A light form of resource virtualization based on kernel mechanisms
A container is a user-space construct
​

Multiple containers run on top of the same kernel
illusion that they are the only one using resources �(cpu, memory, disk, network)
​

some implementations offer support for
container templates
deployment / migration
union filesystems

Google containers (lmctfy)

uses cgroups only, offers CPU & memory isolation
no isolation for: disk I/O, network, filesystem, checkpoint/restore
adds some cgroup files: cpu.lat, cpuacct.histogram
LXC: user-space containerisation tools

Docker

systemd-nspawn

An LXC container is a userspace process created with the clone() system call

​

with its own pid namespace
with its own mnt namespace
net namespace (configurable) - lxc.network.type

Offers container templates /usr/share/lxc/templates

shell scripts
lxc-create -t ubuntu -n containerName
also creates cgroup /sys/fs/cgroup/<controller>/lxc/containerName

For more details
http://lwn.net/Articles/531114/
